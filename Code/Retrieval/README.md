# Retrieval Models

This folder includes implementations for various retrieval models categorized into Sparse Retrieval Models, Unsupervised Dense Retrieval Models, and Supervised Dense Retrieval Models.


## Sparse Retrieval Models

The implementations for **BM25** and **TF-IDF** are located in [./sparse.py](./sparse.py).

## Unspervised Dense Retrieval Models

For unsupervised dense retrieval, we use `SentenceTransformer` to load models like **BGE** and **GTE**. `Faiss` is then used to perform efficient nearest neighbor searches on the vectors generated by the models.

## Spervised Dense Retrieval Models

### Prepare Data Files

Before running the reranking models, ensure the following files are properly prepared:

**Data Folders: fold_{i}**

For each fold (e.g., `fold_1`, `fold_2`, ...), prepare the following JSON files:
- `train.json`
- `valid.json`
- `test.json`

Each file should contain a JSON object formatted like:

```json
{
    "case_id_1": {
        "candidate_dataset_id_1": rel_score,
        "candidate_dataset_id_2": rel_score,
        ...
    },
    "case_id_2": {
        "candidate_dataset_id_3": rel_score,
        "candidate_dataset_id_4": rel_score,
        ...
    },
    ...
}
```

**query_info.json**

This file contains information about each case. The format should be:

```json
{
    "case_id_1": {"content": "case_text_or_info"},
    "case_id_2": {"content": "case_text_or_info"},
    ...
}
```

**dataset_info.json**

This file contains information about each dataset. The format should be:

```json
{
    "dataset_id_1": {"content": "dataset_text_or_info"},
    "dataset_id_2": {"content": "dataset_text_or_info"},
    ...
}
```

### coCondenser

The coCondenser model is implemented using [Tevatron](https://github.com/texttron/tevatron/tree/tevatron-v1). Follow the steps below to install and run it:

1. Install Tevatron with pip:
```bash
git clone -b tevatron-v1 https://github.com/texttron/tevatron
cd tevatron
pip install --editable .
```

2. Run the training command for coCondenser:
```bash
bash pipeline.sh ./bert ./encoding fold_0 1e-5 8 0 20 

```

Parameters:

- `Directory`: Where the processed data is stored.
- `Embedding Directory`: Where the embedded data is stored.
- `Data Folder Directory`: The directory containing the data files.
- `Learning Rate`: The learning rate for training (e.g., 1e-5).
- `Batch Size`: The batch size for training (e.g., 8).
- `Merge Training & Validation Sets`: Whether to merge the training and validation sets (e.g., 0 for no).
- `Top_k Search Results`: Number of top search results to keep (e.g., 20).

The final results will be stored in the `dev.rank.tsv`.

### ColBERTv2

The ColBERTv2 model is implemented using [RAGatouille](https://github.com/AnswerDotAI/RAGatouille). Follow the steps below to install and run it:

1. Install RAGatouille with pip:
```bash
pip install ragatouille
```

2. Run the training command for ColBERTv2:

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 \
python grid_search.py \
--doc_maxlen 512 \
--top_k 20 \
--fold_start 0 \
--fold_end 5 \
--learning_rate 1e-05 \
--batch_size 8 \
--mode train
```

### DPR

The DPR model uses the code reproduced from [Gao et al.](https://github.com/texttron/tevatron/tree/tevatron-v1). It is implemented similarly to coCondenser.

1. Install Tevatron:
```bash
git clone -b tevatron-v1 https://github.com/texttron/tevatron
cd tevatron
pip install --editable .
```

2. Run the training command for DPR:
```
bash pipeline.sh ./bert ./encoding fold_0 1e-5 8 0 20 

```

The parameters for training in DPR are the same as those used in coCondenser.